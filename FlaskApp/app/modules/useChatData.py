import json
import csv
import os
import sqlite3
import pandas as pd
import re
import logging
import unicodedata
import ctypes

# ãƒ­ã‚°è¨­å®šï¼ˆã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ï¼‰
logging.basicConfig(
    filename='error_log.txt',  # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å
    level=logging.ERROR,  # ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒ™ãƒ«ã‚’è¨­å®šï¼ˆERRORãƒ¬ãƒ™ãƒ«ã®ãƒ­ã‚°ã‚’è¨˜éŒ²ï¼‰
    format='%(asctime)s - %(levelname)s - %(message)s',  # ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    encoding="utf-8"  # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’UTF-8ã«è¨­å®š
)


JSON_DIRECTORY = "live_chat"
# SAVE_CHANNEL = "èŠ±èŠ½ã™ã¿ã‚Œ / Nazuna Sumire"
# SAVE_CHANNEL = "èŠ±èŠ½ãªãšãª / Nazuna Kaga"
# SAVE_CHANNEL = "ä¸€ãƒç€¬ã†ã‚‹ã¯"
SAVE_CHANNEL = "èƒ¡æ¡ƒã®ã‚"
# SAVE_CHANNEL = "ç©ºæ¾„ã‚»ãƒŠ -Asumi Sena-"

DB_FILE = "live_chat_comments.db"
FILTERED_DATA = "comments.csv"
GETED_DATA = 'uruha_chat.csv'

# FILTERED_DATA = "sumires.csv"
# DB_FILE = "sumire_member_comments.db"
# GETED_DATA = 'getedSumireData.csv'

# FILTERED_DATA = "yoru.csv"
# DB_FILE = "yoru_comments.db"
# GETED_DATA = 'yoruData.csv'
# SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ¥ç¶šã¨ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆ

def create_db():
    try:
        conn = sqlite3.connect(DB_FILE)
        c = conn.cursor()
        # ã‚³ãƒ¡ãƒ³ãƒˆç”¨ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆï¼ˆurlåˆ—ã¨dateåˆ—ã‚’è¿½åŠ ï¼‰
        c.execute('''      
        CREATE TABLE IF NOT EXISTS comments (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            timestamp INTEGER NOT NULL,
            comment TEXT NOT NULL,
            title TEXT NOT NULL,
            channel TEXT NOT NULL DEFAULT "ä¸€ãƒç€¬ã†ã‚‹ã¯",
            url TEXT,            
            date TEXT            
        )  
        ''')
        conn.commit()
        conn.close()
    except sqlite3.Error as e:
        logging.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ä½œæˆã¾ãŸã¯æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

# JSONã‹ã‚‰ã‚³ãƒ¡ãƒ³ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆã¨æ™‚é–“ã‚’æŠ½å‡º
def extract_comments_from_json(json_file):
    comments = []
    df = getVideoData()
    print(f"å‡¦ç†ä¸­: {json_file}")  # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¡¨ç¤º

    try:
        with open(json_file, "r", encoding="utf-8") as f:
            content = f.read()

            try:
                # è¤‡æ•°ã®JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€1è¡Œãšã¤å‡¦ç†
                data_list = content.splitlines()  # æ”¹è¡Œã§åŒºåˆ‡ã£ã¦è¤‡æ•°ã®JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«åˆ†å‰²

                for line in data_list:
                    try:
                        data = json.loads(line)  # 1è¡Œãšã¤JSONã‚’èª­ã¿è¾¼ã¿

                        actions = data.get("replayChatItemAction", {}).get("actions", [])
                        if not actions:
                            continue  # ã‚³ãƒ¡ãƒ³ãƒˆãŒãªã‘ã‚Œã°æ¬¡ã«é€²ã‚€

                        # ã‚³ãƒ¡ãƒ³ãƒˆéƒ¨åˆ†ã‚’æŠ½å‡º
                        for action in actions:
                            item = action.get("addChatItemAction", {}).get("item", {})
                            renderer = item.get("liveChatTextMessageRenderer", {})
                            message_data = renderer.get("message", {})

                            # "emoji" ã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆã¯ã‚¹ãƒ«ãƒ¼
                            if 'emoji' in message_data:
                                continue

                            # ã‚³ãƒ¡ãƒ³ãƒˆãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                            comment_text = ""
                            for run in message_data.get("runs", []):
                                # "text" ã‚­ãƒ¼ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å‡¦ç†
                                if "text" in run:
                                    comment_text += run["text"]

                            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—
                            timestamp_text = renderer.get("timestampText", {}).get("simpleText", "")

                            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®å½¢å¼ãŒ "hh:mm:ss" ã¾ãŸã¯ "mm:ss" ã®å ´åˆã«å¯¾å¿œ
                            if timestamp_text:
                                # ãƒã‚¤ãƒŠã‚¹ã®æ™‚é–“ã‚’è€ƒæ…®
                                negative = timestamp_text.startswith("-")
                                timestamp_text = timestamp_text.lstrip("-")

                                timestamp_parts = timestamp_text.split(":")
                                if len(timestamp_parts) == 3:  # æ™‚:åˆ†:ç§’å½¢å¼ã®å ´åˆ
                                    hours = int(timestamp_parts[0])
                                    minutes = int(timestamp_parts[1])
                                    seconds = int(timestamp_parts[2])
                                    timestamp = hours * 3600 + minutes * 60 + seconds
                                    if negative:
                                        timestamp = -timestamp
                                elif len(timestamp_parts) == 2:  # åˆ†:ç§’å½¢å¼ã®å ´åˆ
                                    minutes = int(timestamp_parts[0])
                                    seconds = int(timestamp_parts[1])
                                    timestamp = minutes * 60 + seconds
                                    if negative:
                                        timestamp = -timestamp
                                else:
                                    timestamp = 0  # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®å½¢å¼ãŒç•°ãªã‚‹å ´åˆã®å¯¾å¿œ
                            else:
                                timestamp = 0  # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒå–å¾—ã§ããªã‹ã£ãŸå ´åˆ

                            insert_title = os.path.splitext(os.path.basename(json_file))[0].strip().replace('â§¸', '/')

                            # Unicodeæ­£è¦åŒ– (NFKC ã¾ãŸã¯ NFKD ã‚’è©¦ã™)
                            df['Title'] = df['Title'].apply(lambda x: unicodedata.normalize('NFKC', x))
                            insert_title = unicodedata.normalize('NFKC', insert_title)

                            filtered_df = df[df['Title'] == insert_title]

                            # ã‚³ãƒ¡ãƒ³ãƒˆæƒ…å ±ã«ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆtitleï¼‰ã‚’è¿½åŠ 
                            comments.append({
                                "timestamp": timestamp,
                                "comment": comment_text,
                                "title": insert_title,  # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’titleã¨ã—ã¦ä¿å­˜
                                "channel": SAVE_CHANNEL,  # channelã®åˆæœŸå€¤
                                "url": filtered_df['URL'].values[0],
                                "date": filtered_df['date'].values[0]
                            })
                    except json.JSONDecodeError as e:
                        logging.error(f"JSONãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ (è¡Œå‡¦ç†ä¸­): {e}")
                        print(f"JSONãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ (è¡Œå‡¦ç†ä¸­): {e}")
            except json.JSONDecodeError as e:
                logging.error(f"JSONãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
                print(f"JSONãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
    except FileNotFoundError as e:
        logging.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    except Exception as e:
        logging.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    return comments

# ã‚³ãƒ¡ãƒ³ãƒˆã‚’SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
def save_comments_to_db(comments):
    try:
        conn = sqlite3.connect(DB_FILE)
        c = conn.cursor()

        # ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥ï¼ˆurlã¨dateã¯NULLã§ä¿å­˜ï¼‰
        for comment in comments:
            try:
                c.execute('''      
                INSERT INTO comments (timestamp, comment, title, channel, url, date)
                VALUES (?, ?, ?, ?, ?, ?)  
                ''', (comment["timestamp"], comment["comment"], comment["title"], comment["channel"], comment["url"], comment["date"]))
            except sqlite3.Error as e:
                logging.error(f"ã‚³ãƒ¡ãƒ³ãƒˆã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

        conn.commit()
        conn.close()
    except sqlite3.Error as e:
        logging.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
def process_json_files(directory):
    for filename in os.listdir(directory):
        if filename.endswith(".json"):
            json_file = os.path.join(directory, filename)
            
            # æ˜ç¤ºçš„ã« UTF-8 ã«å¤‰æ›
            json_file = json_file.encode("utf-8", "ignore").decode("utf-8")

            print(f"â–¶ {filename} ã‚’å‡¦ç†ä¸­...")

            # JSONã‹ã‚‰ã‚³ãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡º
            comments = extract_comments_from_json(json_file)

            # ã‚³ãƒ¡ãƒ³ãƒˆã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            if comments:
                save_comments_to_db(comments)
            else:
                logging.error(f"ã‚³ãƒ¡ãƒ³ãƒˆãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ: {filename}")

    print("ğŸ‰ ã™ã¹ã¦ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã¾ã—ãŸï¼")

def getVideoData():
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    try:
        df = pd.read_csv(GETED_DATA)
        return df
    except FileNotFoundError as e:
        logging.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
        return pd.DataFrame()  # ç©ºã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿”ã™

def main():
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ä½œæˆ
    create_db()

    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®š
    json_directory = JSON_DIRECTORY  # ã“ã“ã«JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’æŒ‡å®š

    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    process_json_files(json_directory)


def rename_json():
    import os
    import re

    # å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’æŒ‡å®š
    target_directory = "live_chat"

    # ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    pattern = re.compile(r" \[[a-zA-Z0-9_-]+\]\.live_chat\.json$")

    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã—ã¦ãƒªãƒãƒ¼ãƒ 
    for filename in os.listdir(target_directory):
        old_path = os.path.join(target_directory, filename)
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å¯¾è±¡ã«ã™ã‚‹
        if filename.endswith(".json"):
            new_filename = pattern.sub(".json", filename)  # è©²å½“éƒ¨åˆ†ã‚’å‰Šé™¤
            new_path = os.path.join(target_directory, new_filename)

            # æ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if old_path != new_path and not os.path.exists(new_path):
                os.rename(old_path, new_path)
                print(f"Renamed: {filename} -> {new_filename}")
            elif old_path != new_path:
                print(f"Skipped (file exists): {new_filename}")

def remove_duplicates():
    try:
        conn = sqlite3.connect(DB_FILE)
        c = conn.cursor()
        
        # é‡è¤‡ã™ã‚‹è¡Œã‚’å‰Šé™¤ï¼ˆidä»¥å¤–ã®ã™ã¹ã¦ã®åˆ—ãŒé‡è¤‡ã—ã¦ã„ã‚‹è¡Œï¼‰
        c.execute('''
            DELETE FROM comments
            WHERE id NOT IN (
                SELECT MIN(id)
                FROM comments
                GROUP BY timestamp, comment, title, channel, url, date
            );
        ''')
        conn.commit()
        conn.close()
    except sqlite3.Error as e:
        logging.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰é‡è¤‡ã‚’å‰Šé™¤ã™ã‚‹éš›ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


def count_rows():
    try:
        conn = sqlite3.connect(DB_FILE)
        c = conn.cursor()
        # è¡Œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        c.execute('SELECT COUNT(*) FROM comments')
        row_count = c.fetchone()[0]
        conn.close()
        print(f"ã‚³ãƒ¡ãƒ³ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡Œæ•°: {row_count}")
        return row_count
    except sqlite3.Error as e:
        print(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def search_comments(channel=None, title=None, date=None, comment=None):
    try:
        conn = sqlite3.connect(DB_FILE)
        c = conn.cursor()

        # åŸºæœ¬çš„ãªæ¤œç´¢ã‚¯ã‚¨ãƒª
        query = "SELECT * FROM comments WHERE 1=1"
        params = []

        # ãƒãƒ£ãƒ³ãƒãƒ«ã§çµã‚Šè¾¼ã¿ï¼ˆæ¡ä»¶ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°ï¼‰
        if channel:
            query += " AND channel = ?"
            params.append(channel)

        # ã‚¿ã‚¤ãƒˆãƒ«ã§çµã‚Šè¾¼ã¿ï¼ˆæ¡ä»¶ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°ï¼‰
        if title:
            query += " AND title LIKE ?"
            params.append(f"%{title}%")  # éƒ¨åˆ†ä¸€è‡´æ¤œç´¢

        # æ—¥ä»˜ã§çµã‚Šè¾¼ã¿ï¼ˆæ¡ä»¶ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°ï¼‰
        if date:
            query += " AND date = ?"
            params.append(date)

        # ã‚³ãƒ¡ãƒ³ãƒˆã§çµã‚Šè¾¼ã¿ï¼ˆæ¡ä»¶ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°ï¼‰
        if comment:
            query += " AND comment LIKE ?"
            params.append(f"%{comment}%")  # éƒ¨åˆ†ä¸€è‡´æ¤œç´¢

        # ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
        c.execute(query, params)
        results = c.fetchall()

        # çµæœãŒã‚ã‚Œã°ãƒªã‚¹ãƒˆã§è¿”ã™
        conn.close()

        if results:
            return results  # çµæœã‚’è¿”ã™
        else:
            return []  # è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™

    except sqlite3.Error as e:
        logging.error(f"æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return []  # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã‚‚ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™

def save_to_csv(data, filename=FILTERED_DATA):
    if not data:
        logging.warning("ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    try:
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ãï¼ˆè¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ï¼‰
        with open(filename, mode="w", newline='', encoding="utf-8") as file:
            writer = csv.writer(file)

            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’æ›¸ãè¾¼ã‚€
            writer.writerow(["ID", "Timestamp", "Comment", "Title", "Channel", "URL", "Date"])

            # ãƒ‡ãƒ¼ã‚¿è¡Œã‚’æ›¸ãè¾¼ã‚€
            for row in data:
                writer.writerow(row)

        logging.info(f"ãƒ‡ãƒ¼ã‚¿ãŒ {filename} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
    
    except Exception as e:
        logging.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

def get_oldest_and_latest_comment(channel_name):
    """
    ä½¿ã„æ–¹
    oldest_comment, latest_comment = get_oldest_and_latest_comment("ä¸€ãƒç€¬ã†ã‚‹ã¯")
    print("æœ€ã‚‚å¤ã„ã‚³ãƒ¡ãƒ³ãƒˆ:", oldest_comment)
    print("æœ€ã‚‚æ–°ã—ã„ã‚³ãƒ¡ãƒ³ãƒˆ:", latest_comment)
    """
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()

    # æœ€ã‚‚æ—©ã„ `date`
    c.execute('''
        SELECT * FROM comments 
        WHERE channel = ? 
        ORDER BY date ASC 
        LIMIT 1
    ''', (channel_name,))
    oldest = c.fetchone()

    # æœ€ã‚‚é…ã„ `date`
    c.execute('''
        SELECT * FROM comments 
        WHERE channel = ? 
        ORDER BY date DESC 
        LIMIT 1
    ''', (channel_name,))
    latest = c.fetchone()

    conn.close()
    return oldest, latest

def update_channel_by_url(url, new_channel):
    """
    ä½¿ã„æ–¹
    update_channel_by_url("https://example.com/video123", "æ–°ã—ã„ãƒãƒ£ãƒ³ãƒãƒ«å")
    """
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()

    c.execute('''
        UPDATE comments 
        SET channel = ? 
        WHERE url = ?
    ''', (new_channel, url))

    conn.commit()
    conn.close()

def delete_json_files(directory):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã€
    ã‚´ãƒŸç®±ã‚‚ç©ºã«ã™ã‚‹é–¢æ•°ã€‚
    
    Parameters:
    directory (str): JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã™ã‚‹å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
    """
    # 1. æŒ‡å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    for filename in os.listdir(directory):
        if filename.endswith(".json"):
            json_file = os.path.join(directory, filename)
            try:
                os.remove(json_file)
                print(f"å‰Šé™¤ã—ã¾ã—ãŸ: {json_file}")
            except Exception as e:
                print(f"{json_file} ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    # 2. ã‚´ãƒŸç®±ã‚’ç©ºã«ã™ã‚‹
    ctypes.windll.shell32.SHEmptyRecycleBinW(0, None, 1)
    print("ã‚´ãƒŸç®±ã‚’ç©ºã«ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    # # delete_json_files(JSON_DIRECTORY)
    # rename_json()
    # main()
    # # # remove_duplicates()
    # # count_rows()
    # response = search_comments(channel=SAVE_CHANNEL, comment="è…¹ç­‹")
    # save_to_csv(response)
    # # oldest, latest = get_oldest_and_latest_comment("èŠ±èŠ½ã™ã¿ã‚Œ / Kaga Sumire")
    oldest, latest = get_oldest_and_latest_comment("èƒ¡æ¡ƒã®ã‚")
    print(oldest)
